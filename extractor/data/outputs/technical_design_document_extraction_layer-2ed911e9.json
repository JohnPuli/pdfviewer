[
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p1_c1",
    "page":1,
    "bbox_normalized":[
      0.155147,
      0.096894,
      0.844853,
      0.155967
    ],
    "bbox_abs":[
      94.9499969482,
      76.7399978638,
      517.0499267578,
      123.5260009766
    ],
    "text":"Technical Design Document: Extraction Layer for Side-by-Side PDF Viewer",
    "created_at":"2026-01-05T09:12:23.075483"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p1_c2",
    "page":1,
    "bbox_normalized":[
      0.127451,
      0.202803,
      0.254618,
      0.227144
    ],
    "bbox_abs":[
      78.0,
      160.6199798584,
      155.8260040283,
      179.8979797363
    ],
    "text":"1. Overview",
    "created_at":"2026-01-05T09:12:23.075502"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p1_c3",
    "page":1,
    "bbox_normalized":[
      0.127451,
      0.233396,
      0.852238,
      0.296199
    ],
    "bbox_abs":[
      78.0,
      184.8499755859,
      521.5698852539,
      234.5899810791
    ],
    "text":"The extraction layer is a backend subsystem responsible for transforming raw PDFs into structured data used by the side-by-side PDF viewer. It powers text extraction, layout mapping, semantic chunking, embedding generation, and metadata storage. This document outlines the full engineering architecture, workflow, components, and integration points for the extraction layer.",
    "created_at":"2026-01-05T09:12:23.075510"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p1_c4",
    "page":1,
    "bbox_normalized":[
      0.127451,
      0.331591,
      0.510118,
      0.355932
    ],
    "bbox_abs":[
      78.0,
      262.6199951172,
      312.1920166016,
      281.8980102539
    ],
    "text":"2. System Goals and Requirements",
    "created_at":"2026-01-05T09:12:23.075517"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p1_c5",
    "page":1,
    "bbox_normalized":[
      0.127451,
      0.362184,
      0.867255,
      0.470442
    ],
    "bbox_abs":[
      78.0,
      286.8500061035,
      530.7598266602,
      372.5899963379
    ],
    "text":"Functional Requirements: \u2022 Convert PDFs into structured chunks containing text, page, bounding boxes, and metadata. \u2022 Normalize bounding boxes for consistent rendering in web viewers. \u2022 Generate embeddings for semantic search and retrieval. \u2022 Store structured outputs in the data lake, SQL database, and vector index. \u2022 Support incremental and idempotent processing. Non-Functional Requirements: \u2022 Scalable and parallelizable processing pipeline. \u2022 Fault tolerance and retry logic. \u2022 Efficient batch processing for embeddings. \u2022 Cloud storage integration (S3\/Azure\/GCS). \u2022 Low-latency metadata retrieval for the viewer.",
    "created_at":"2026-01-05T09:12:23.075523"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p1_c6",
    "page":1,
    "bbox_normalized":[
      0.127451,
      0.505833,
      0.619395,
      0.530174
    ],
    "bbox_abs":[
      78.0,
      400.6199951172,
      379.0699157715,
      419.8980102539
    ],
    "text":"3. Architecture Diagram (Textual Description)",
    "created_at":"2026-01-05T09:12:23.075531"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p1_c7",
    "page":1,
    "bbox_normalized":[
      0.127451,
      0.536427,
      0.854199,
      0.629533
    ],
    "bbox_abs":[
      78.0,
      424.8500061035,
      522.7699584961,
      498.5899963379
    ],
    "text":"The system follows a multi-layer architecture: User Upload  \u2192  Ingestion API  \u2192  Data Lake Storage  \u2192 Extraction Worker  \u2192  Metadata Outputs  \u2192  SQL DB + Vector DB  \u2192  Viewer Modules: \u2022 Ingestion Service \u2013 Stores original PDFs and creates extraction jobs. \u2022 Extraction Worker \u2013 Performs parsing, normalization, chunking, embedding. \u2022 Metadata Store \u2013 Parquet in data lake, plus relational index. \u2022 Vector Index \u2013 Embeddings for semantic retrieval. \u2022 Viewer \u2013 Consumes metadata and provides interactive experiences.",
    "created_at":"2026-01-05T09:12:23.075537"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p1_c8",
    "page":1,
    "bbox_normalized":[
      0.127451,
      0.664924,
      0.478297,
      0.689265
    ],
    "bbox_abs":[
      78.0,
      526.6199951172,
      292.7180175781,
      545.8979492188
    ],
    "text":"4. Extraction Layer Components",
    "created_at":"2026-01-05T09:12:23.075544"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p1_c9",
    "page":1,
    "bbox_normalized":[
      0.127451,
      0.695518,
      0.860555,
      0.788624
    ],
    "bbox_abs":[
      78.0,
      550.8499755859,
      526.6598510742,
      624.5899658203
    ],
    "text":"4.1 PDF Loader Loads PDFs from storage using PyMuPDF or similar. 4.2 Layout Extractor Extracts: \u2022 Text blocks \u2022 Bounding boxes \u2022 Page dimensions \u2022 Formatting markers 4.3 Normalization Module Converts PDF coordinates into normalized values aligned with browser coordinate systems. 4.4 Chunker Segments text into semantic chunks with identifiers. 4.5 Embedding Generator Produces embeddings in batch for semantic search. 4.6 Metadata Packager Packages all structured elements and writes to data lake, SQL DB, and vector DB.",
    "created_at":"2026-01-05T09:12:23.075549"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p1_c10",
    "page":1,
    "bbox_normalized":[
      0.127451,
      0.824015,
      0.287627,
      0.848356
    ],
    "bbox_abs":[
      78.0,
      652.6199951172,
      176.0280151367,
      671.8979492188
    ],
    "text":"5. Data Models",
    "created_at":"2026-01-05T09:12:23.075554"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p1_c11",
    "page":1,
    "bbox_normalized":[
      0.127451,
      0.854609,
      0.86915,
      0.90226
    ],
    "bbox_abs":[
      78.0,
      676.8499755859,
      531.9198608398,
      714.5899658203
    ],
    "text":"Chunk Metadata Schema: \u2022 chunk_id: string \u2022 doc_id: string \u2022 page: int \u2022 text: string \u2022 bbox_normalized: [float, float, float, float] \u2022 bbox_points: [float, float, float, float] \u2022 embedding: vector (optional at ingestion) \u2022 chunk_hash: string SQL Database Tables: \u2022 documents \u2022 chunks (chunk_id, doc_id, page, bbox,",
    "created_at":"2026-01-05T09:12:23.075559"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c1",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.097538,
      0.720964,
      0.114886
    ],
    "bbox_abs":[
      78.0,
      77.25,
      441.2299194336,
      90.9899978638
    ],
    "text":"preview_text) Vector DB Metadata: \u2022 embedding \u2022 doc_id \u2022 chunk_id \u2022 bbox \u2022 page",
    "created_at":"2026-01-05T09:12:23.080715"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c2",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.150278,
      0.417265,
      0.174619
    ],
    "bbox_abs":[
      78.0,
      119.0199966431,
      255.3659973145,
      138.2980041504
    ],
    "text":"6. Workflow and Data Flow",
    "created_at":"2026-01-05T09:12:23.080722"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c3",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.180871,
      0.869395,
      0.258826
    ],
    "bbox_abs":[
      78.0,
      143.25,
      532.0698852539,
      204.9900054932
    ],
    "text":"1. PDF Upload triggers job creation. 2. Extraction worker loads PDF from data lake. 3. Layout extractor parses blocks and coordinates. 4. Normalizer converts box coordinates. 5. Chunker creates semantic chunk units. 6. Embedding generator creates vector embeddings in batches. 7. Data is persisted: \u2022 Parquet files in data lake \u2022 SQL DB rows for quick lookups \u2022 Vector DB entries for search 8. Viewer fetches SQL metadata and renders PDF with highlight overlays.",
    "created_at":"2026-01-05T09:12:23.080725"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c4",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.294217,
      0.465601,
      0.318558
    ],
    "bbox_abs":[
      78.0,
      233.0200042725,
      284.9479980469,
      252.2980041504
    ],
    "text":"7. Performance Considerations",
    "created_at":"2026-01-05T09:12:23.080728"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c5",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.324811,
      0.841797,
      0.387614
    ],
    "bbox_abs":[
      78.0,
      257.25,
      515.1798706055,
      306.9899902344
    ],
    "text":"\u2022 Parallelize processing on page or document level. \u2022 Batch embeddings to reduce API overhead. \u2022 Cache metadata in Redis for low-latency viewer loads. \u2022 Use chunk hashing to skip redundant processing. \u2022 Implement retry logic for embedding API failures. \u2022 Use Parquet for efficient metadata storage and analytics.",
    "created_at":"2026-01-05T09:12:23.080731"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c6",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.423005,
      0.468118,
      0.447346
    ],
    "bbox_abs":[
      78.0,
      335.0199890137,
      286.4880065918,
      354.2980041504
    ],
    "text":"8. Failure Modes and Mitigation",
    "created_at":"2026-01-05T09:12:23.080734"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c7",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.453598,
      0.871127,
      0.516402
    ],
    "bbox_abs":[
      78.0,
      359.25,
      533.1298217773,
      408.9899902344
    ],
    "text":"Potential Issues: \u2022 Damaged PDFs \u2022 Missing text due to scanned pages \u2022 OCR failures \u2022 Embedding generation timeouts \u2022 Inconsistent bounding boxes Mitigations: \u2022 OCR fallback pipeline using Tesseract\/Vision API. \u2022 Page-level retries. \u2022 Hash-based detection to resume failed jobs. \u2022 Dual storage of raw and normalized bounding boxes for debugging. \u2022 Validation suite with overlay previews.",
    "created_at":"2026-01-05T09:12:23.080737"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c8",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.551793,
      0.416029,
      0.576134
    ],
    "bbox_abs":[
      78.0,
      437.0199890137,
      254.6100006104,
      456.2980041504
    ],
    "text":"9. Security Considerations",
    "created_at":"2026-01-05T09:12:23.080740"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c9",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.582386,
      0.842026,
      0.614886
    ],
    "bbox_abs":[
      78.0,
      461.25,
      515.3198852539,
      486.9899902344
    ],
    "text":"\u2022 Use signed URLs for PDF upload and retrieval. \u2022 Encrypt data lake buckets. \u2022 Restrict worker IAM permissions. \u2022 Sanitize text for unsafe characters. \u2022 Log access for audit compliance.",
    "created_at":"2026-01-05T09:12:23.080742"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c10",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.650278,
      0.409647,
      0.674619
    ],
    "bbox_abs":[
      78.0,
      515.0200195312,
      250.7040100098,
      534.2979736328
    ],
    "text":"10. Deployment Approach",
    "created_at":"2026-01-05T09:12:23.080745"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c11",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.680871,
      0.856291,
      0.728523
    ],
    "bbox_abs":[
      78.0,
      539.25,
      524.0498657227,
      576.9899902344
    ],
    "text":"Deployment Options: \u2022 Dockerized processing worker. \u2022 Container orchestration: Kubernetes or AWS ECS. \u2022 Event-driven triggers via S3\/Azure Blob events. \u2022 CI\/CD for pipeline updates. \u2022 Autoscaling based on queue depth.",
    "created_at":"2026-01-05T09:12:23.080748"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c12",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.763914,
      0.34484,
      0.788255
    ],
    "bbox_abs":[
      78.0,
      605.0200195312,
      211.0420074463,
      624.2979736328
    ],
    "text":"11. Testing Strategy",
    "created_at":"2026-01-05T09:12:23.080750"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c13",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.794508,
      0.857778,
      0.842159
    ],
    "bbox_abs":[
      78.0,
      629.25,
      524.9598388672,
      666.9899902344
    ],
    "text":"Unit Tests: \u2022 bbox normalization correctness \u2022 chunk segmentation logic \u2022 embedding vector lengths \u2022 metadata schema validation Integration Tests: \u2022 full PDF-to-chunk pipeline \u2022 vector DB search correctness Load Tests: \u2022 large document ingestion \u2022 parallel extraction performance",
    "created_at":"2026-01-05T09:12:23.080753"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p2_c14",
    "page":2,
    "bbox_normalized":[
      0.127451,
      0.877551,
      0.290167,
      0.901891
    ],
    "bbox_abs":[
      78.0,
      695.0200195312,
      177.5820159912,
      714.2979736328
    ],
    "text":"12. Conclusion",
    "created_at":"2026-01-05T09:12:23.080756"
  },
  {
    "chunk_id":"technical_design_document_extraction_layer-2ed911e9_p3_c1",
    "page":3,
    "bbox_normalized":[
      0.127451,
      0.097538,
      0.858497,
      0.175492
    ],
    "bbox_abs":[
      78.0,
      77.25,
      525.3999023438,
      138.9900054932
    ],
    "text":"The extraction layer is the backbone of the intelligent PDF experience. It transforms raw PDFs into structured, searchable, interactive documents. By integrating layout analysis, normalization, semantic chunking, and embeddings, it enables the side-by-side viewer to deliver a powerful AI-augmented reading and annotation experience. This technical design ensures scalability, precision, and maintainability across large document processing workloads.",
    "created_at":"2026-01-05T09:12:23.081868"
  }
]